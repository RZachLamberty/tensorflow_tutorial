{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using gpus\n",
    "\n",
    "following along with [this](https://www.tensorflow.org/programmers_guide/using_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supported devices\n",
    "\n",
    "supported devices are `CPU` and `GPU` (note: `TPU` is not supported in the same way; see neighboring notebook)\n",
    "\n",
    "if both exist and an operation *can* be executed on a gpu, then gpu devices will be given preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging device placement\n",
    "\n",
    "logging of device placement is a configuration option that you can set in the `config` of a tensorflow session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as with a previous logging experience, log messages are written to `stdout` and therefore unavilable for we `jupyter notebook` plebes. I ran that code in the terminal and the ouptut was:\n",
    "\n",
    "```\n",
    "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 12:45:48.273487: I tensorflow/core/common_runtime/placer.cc:886] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "b: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 12:45:48.273501: I tensorflow/core/common_runtime/placer.cc:886] b: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "a: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 12:45:48.273509: I tensorflow/core/common_runtime/placer.cc:886] a: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "[[22. 28.]\n",
    " [49. 64.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual device placement\n",
    "\n",
    "sometimes we might want to control the device(s) on which computation happens. we can do this with a `tf.device` context manager:\n",
    "\n",
    "```python\n",
    "# Creates a graph.\n",
    "with tf.device('/cpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print(sess.run(c))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when I run that from the cli, I get the following additional log messages:\n",
    "\n",
    "```\n",
    "Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
    "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN V, pci bus id: 0000:05:00.0, compute capability: 7.0\n",
    "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: TITAN V, pci bus id: 0000:09:00.0, compute capability: 7.0\n",
    "2018-07-04 15:09:41.394792: I tensorflow/core/common_runtime/direct_session.cc:284] Device mapping:\n",
    "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
    "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
    "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN V, pci bus id: 0000:05:00.0, compute capability: 7.0\n",
    "/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: TITAN V, pci bus id: 0000:09:00.0, compute capability: 7.0\n",
    "\n",
    "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 15:09:41.395242: I tensorflow/core/common_runtime/placer.cc:886] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "b: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-04 15:09:41.395257: I tensorflow/core/common_runtime/placer.cc:886] b: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "a: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-04 15:09:41.395264: I tensorflow/core/common_runtime/placer.cc:886] a: (Const)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we see that the computation of tensors `a` and `b` is logged as remaining on the cpu 0 even though the `matmul` operation is promoted to the `gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## allowing gpu memory growth\n",
    "\n",
    "default gpu memory usage is to obtain a lock on as much of the visible gpu memory as is possible. you can change this with the `config.gpu_options.allow_growth` paramet as follows:\n",
    "\n",
    "```python\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# true to allow growth from small number,\n",
    "# false to take as much as is available immediately\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config, ...) as sess:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, you could change the overall fraction of the GPU memory a process is allowed to consume with the `config.gpu_options.per_process_gpu_memory_fraction` parameter:\n",
    "\n",
    "```python\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using a single gpu on a multi-gpu system\n",
    "\n",
    "you can specify a single gpu by index. this is covered about a million times. again, `tf.device` context manager. example:\n",
    "\n",
    "```python\n",
    "# Creates a graph.\n",
    "with tf.device('/device:GPU:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print(sess.run(c))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if `gpu:N` doesn't exist, you're get an error (oddly, not when the tensor is assigned, but instead when the operation is calculated. not sure why that would be):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "see, there it is!\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "with tf.device('/device:GPU:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "# Creates a session with log_device_placement set to True.\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    try:\n",
    "        print(sess.run(c))\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print('see, there it is!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interestingly, they have a second configuration option for sessions `allow_soft_placement` that will attempt to place the items on the requested device but will safely fall back in the event of an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "with tf.device('/device:GPU:2'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "# Creates a session with log_device_placement set to True.\n",
    "config = tf.ConfigProto(\n",
    "    allow_soft_placement=True,\n",
    "    log_device_placement=True\n",
    ")\n",
    "with tf.Session(config=config) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using multiple gpus\n",
    "\n",
    "in order to use multiple gpus, they recomend a \"multi-tower\" paradigm:\n",
    "\n",
    "```python\n",
    "c = []\n",
    "for d in ['/device:GPU:0', '/device:GPU:1']:\n",
    "    with tf.device(d):\n",
    "        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "        c.append(tf.matmul(a, b))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    s = tf.add_n(c)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    print(sess.run(s))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, running this in a terminal which has two gpus exposed I get\n",
    "\n",
    "```\n",
    "MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:1\n",
    "2018-07-04 15:42:48.835288: I tensorflow/core/common_runtime/placer.cc:886] MatMul_1: (MatMul)/job:localhost/replica:0/task:0/device:GPU:1\n",
    "MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 15:42:48.835305: I tensorflow/core/common_runtime/placer.cc:886] MatMul: (MatMul)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "AddN: (AddN): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "2018-07-04 15:42:48.835314: I tensorflow/core/common_runtime/placer.cc:886] AddN: (AddN)/job:localhost/replica:0/task:0/device:CPU:0\n",
    "Const_3: (Const): /job:localhost/replica:0/task:0/device:GPU:1\n",
    "2018-07-04 15:42:48.835322: I tensorflow/core/common_runtime/placer.cc:886] Const_3: (Const)/job:localhost/replica:0/task:0/device:GPU:1\n",
    "Const_2: (Const): /job:localhost/replica:0/task:0/device:GPU:1\n",
    "2018-07-04 15:42:48.835330: I tensorflow/core/common_runtime/placer.cc:886] Const_2: (Const)/job:localhost/replica:0/task:0/device:GPU:1\n",
    "Const_1: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 15:42:48.835338: I tensorflow/core/common_runtime/placer.cc:886] Const_1: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0\n",
    "2018-07-04 15:42:48.835345: I tensorflow/core/common_runtime/placer.cc:886] Const: (Const)/job:localhost/replica:0/task:0/device:GPU:0\n",
    "[[ 44.  56.]\n",
    " [ 98. 128.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "not a lot of new information here\n",
    "\n",
    "+ if you want to use a specific `cpu` or `gpu`, use a `tf.device` context manager around your variable declarations\n",
    "+ you can log device information with a configuration variable `log_device_placement`\n",
    "+ you can be permissive about failure to allocate devices with `allow_soft_placement`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
