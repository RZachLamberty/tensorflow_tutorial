{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using tpus\n",
    "\n",
    "following along with [this](https://www.tensorflow.org/programmers_guide/using_tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they make specific reference to cloud `tpu`s, as if the real deal doesn't exist. maybe that's true, not sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TPUEstimator`\n",
    "\n",
    "all standard estimator objects are implemented on `cpu` and `gpu` *only*. if you want to use `tpu`s, you have to convert those estimators to an entirely different object: `tf.contrib.tpu.TPUEstimator`\n",
    "\n",
    "why is that? seems like it has to be on the roadmap for them to make this entirely equivalent to the cpu/gpu paradigm, right? confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.contrib.tpu.TPUEstimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the difference between this class and the basic estimators is significant enough that they suggest an architectural solution: if you want a model to be runnable under both cpu/gpu and tpu frameworks, make a fundamental abstraction / conceptual separation:\n",
    "\n",
    "> define the model's inference phase (from inputs to predictions) outside of the `model_fn`. Then maintain separate implementations of the Estimator setup and `model_fn`, both wrapping this inference step\n",
    "\n",
    "for what it's worth, I looked at their example of \"how to do this\" and it didn't mean anything to me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running a `TPUEstimator` locally\n",
    "\n",
    "it would suck if you had to have a tpu to develop a tpu project; fortunately for development's sake you can \"turn off\" tpus for the `TPUEstimator` class by setting `use_tpu=False` and creating a config:\n",
    "\n",
    "```python\n",
    "my_tpu_estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    model_fn=my_model_fn,\n",
    "    config=tf.contrib.tpu.RunConfig()\n",
    "    use_tpu=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building a `tpu.RunConfig`\n",
    "\n",
    "speaking of that configuration, the `tf.contrib.tpu.RunConfig()` is a bare-bones configuraiton file for tpu estimator sessions. let's checkit out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_cluster',\n",
       " '_cluster_spec',\n",
       " '_evaluation_master',\n",
       " '_global_id_in_cluster',\n",
       " '_init_distributed_setting_from_environment_var',\n",
       " '_init_distributed_setting_from_environment_var_with_master',\n",
       " '_is_chief',\n",
       " '_keep_checkpoint_every_n_hours',\n",
       " '_keep_checkpoint_max',\n",
       " '_log_step_count_steps',\n",
       " '_master',\n",
       " '_model_dir',\n",
       " '_num_ps_replicas',\n",
       " '_num_worker_replicas',\n",
       " '_replace',\n",
       " '_save_checkpoints_secs',\n",
       " '_save_checkpoints_steps',\n",
       " '_save_summary_steps',\n",
       " '_service',\n",
       " '_session_config',\n",
       " '_task_id',\n",
       " '_task_type',\n",
       " '_tf_api_names',\n",
       " '_tf_random_seed',\n",
       " '_tpu_config',\n",
       " '_train_distribute',\n",
       " 'cluster',\n",
       " 'cluster_spec',\n",
       " 'evaluation_master',\n",
       " 'global_id_in_cluster',\n",
       " 'is_chief',\n",
       " 'keep_checkpoint_every_n_hours',\n",
       " 'keep_checkpoint_max',\n",
       " 'log_step_count_steps',\n",
       " 'master',\n",
       " 'model_dir',\n",
       " 'num_ps_replicas',\n",
       " 'num_worker_replicas',\n",
       " 'replace',\n",
       " 'save_checkpoints_secs',\n",
       " 'save_checkpoints_steps',\n",
       " 'save_summary_steps',\n",
       " 'service',\n",
       " 'session_config',\n",
       " 'task_id',\n",
       " 'task_type',\n",
       " 'tf_random_seed',\n",
       " 'tpu_config',\n",
       " 'train_distribute']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.contrib.tpu.RunConfig()\n",
    "[_ for _ in dir(config) if _[:2] != '__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TPUConfig(iterations_per_loop=2, num_shards=None, computation_shape=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.tpu_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is possible and indeed generally necessary to update the configuration of these tpu estimator objects. the docs here provide a whole walkthrough on how to creat a custom `FLAGS` object, parameterize the attributes of that `FLAG` at runtime, and pass them in as conscious semi-automated parameterization of the most important parts of a `tf.contrib.tpu.RunConfig`. it's simple stuff but so, so, so engineered. I have to imagine they could have defined a `yaml` of `conf` that would have taken care of all of this, which begs the question: why didn't they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer\n",
    "\n",
    "the built-in optimizers don't work on clout tpus, and vice versa. a common pattern is:\n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "if FLAGS.use_tpu:\n",
    "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model function\n",
    "\n",
    "you're not done yet. your pretty little `model_fn` has to change, too:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### static shapes\n",
    "\n",
    "cloud tpu calculations use XLA (Accelerated Linear Algebra) -- an alpha technology, fwiw -- to do it's linear algebar calculations.\n",
    "\n",
    "XLA requires compile-time knowledge about shapes. you must update your code to have statically-shaped inputs and outputs. bummer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summaries\n",
    "\n",
    "remove all references to `tf.Summary`. it's not supported yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics\n",
    "\n",
    "there are differences in the esimator specs that make it such that your metric functions must have different signatures in the two different paradigms.\n",
    "\n",
    "in regular cpu/gpu estimators, the `metric_fn` returns a an `EstimatorSpec` which requires the user to specify the `eval_metrics_ops`:\n",
    "\n",
    "```python\n",
    "my_metrics = {'accuracy': tf.metrics.accuracy(labels, predictions)}\n",
    "\n",
    "return tf.estimator.EstimatorSpec(\n",
    "    ...\n",
    "    eval_metric_ops=my_metrics\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for tpu estimators, the spec object is a `tf.contrib.tpu.TPUEstimatorSpec`. instead of requesting a dictionary of operations `eval_metric_ops` as in the regular estimator case, it requests a *function* for creating that dictionary, and an iterable of the tensor arguments to that function. to generalize the piece above:\n",
    "\n",
    "```python\n",
    "def my_metric_fn(labels, predictions):\n",
    "     return {'accuracy': tf.metrics.accuracy(labels, predictions)}\n",
    "\n",
    "return tf.contrib.tpu.TPUEstimatorSpec(\n",
    "    ...\n",
    "    eval_metrics=(my_metric_fn, [labels, predictions])\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this seems extremely fucking silly. why not just have the same interface? this is bonkers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use `TPUEstimatorSpec`\n",
    "\n",
    "one of the big differences between the `TPUEstimatorSpec` and the cpu/gpu `EstimatorSpec` is the way it expects metric operation to be defined (c.f. prev section). others include:\n",
    "\n",
    "1. `hooks` (haven't covered yet) are not supported\n",
    "1. `scaffold` is converted in much the same way as metrics (instead of receiving a dictionary, you promote it to a function that returns a dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input functions\n",
    "\n",
    "all of the changes up above were major changes required to modify the behavior to the code when running on a TPU. the *input* process usually happens on the host computer, so not much must change, right? well, there's still some shit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### params argument\n",
    "\n",
    "`Estimator` `input_fn`s *can* have a `params` argument; `TPUEstimator` `input_fn`s *must* have a `params` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### static shapes and batch size\n",
    "\n",
    "as mentioned above, XLA requires known shapes. if the shape inference of your input pipeline / `input_fn` fails to resolve the shape of input tensors, you can mandate it using the `tf.set_shape` function\n",
    "\n",
    "for batch sizes it's trickier; your dataset might not be an even multiple of your batch size. in these instances, you are advised to use `tf.contrib.data.batch_and_drop_remainder` and deal with it. if you can't deal with it, pad the final batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasets\n",
    "\n",
    "you have to jump through hoops to stream data to this cloud service (basically, streaming data is too slow for the TPU calculation, and bandwidth is too big a bottleneck). so upload your stuff in `TFRecord` format to google cloud buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what next\n",
    "\n",
    "extra documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "+ if you want to use google cloud tpu, you probably better make sure\n",
    "+ you have to make changes to the implementation of your code\n",
    "+ the interfaces are *not* comparable, so the changes are not trivial. they are easy, but require some hacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
