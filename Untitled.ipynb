{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variables\n",
    "\n",
    "following along with [this page](https://www.tensorflow.org/programmers_guide/variables)\n",
    "\n",
    "a variable is a way to \"represent sharedm persistent state\", which is not a bad idea\n",
    "\n",
    "they are defined by the `tf.Variable` class, which is simply a mutable subclass of `tf.Tensor`. another important difference: most `tf.Tensor` objects exist only within a single `tf.Session.run` call, a variable exists outside them as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating a variable\n",
    "\n",
    "easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_variable:0' shape=(1, 2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_variable = tf.get_variable(\n",
    "    name='my_variable',\n",
    "    shape=[1, 2, 3]\n",
    ")\n",
    "my_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.0836813  -0.8812319   0.80602884]\n",
      "  [-0.4607755  -0.9072799  -0.53925633]]]\n"
     ]
    }
   ],
   "source": [
    "utils.inspect(my_variable, init_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other params for `tf.get_variable` include `dtype`, `initializer`, `regularizer`, blah blah blah full docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_int_variable:0' shape=(1, 2, 3) dtype=int32_ref>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_int_variable = tf.get_variable(\n",
    "    name='my_int_variable',\n",
    "    shape=[1, 2, 3],\n",
    "    dtype=tf.int32,\n",
    "    initializer=tf.zeros_initializer\n",
    ")\n",
    "my_int_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "utils.inspect(my_int_variable, init_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are \"many\" initializers (full section on it below). a common one is to initialize to a known tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'other_variable:0' shape=(2,) dtype=int32_ref>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_variable = tf.get_variable(\n",
    "    \"other_variable\",\n",
    "    # note: no shape=, because shape is / must be inferred from initializer tensor\n",
    "    dtype=tf.int32,\n",
    "    initializer=tf.constant([23, 42])\n",
    ")\n",
    "other_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23 42]\n"
     ]
    }
   ],
   "source": [
    "utils.inspect(other_variable, init_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable collections\n",
    "\n",
    "we've seen and accessed collections before, but this is the first direct discussion of them. they write:\n",
    "\n",
    "> Because disconnected parts of a TensorFlow program might want to create variables, it is sometimes useful to have a single way to access all of them. For this reason TensorFlow provides collections, which are named lists of tensors or other objects, such as `tf.Variable` instances.\n",
    "\n",
    "so `collections` are ways for users to group shared variables together *a la* namespaces.\n",
    "\n",
    "by default, everything goes in the following two collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_variable:0' shape=(1, 2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'my_int_variable:0' shape=(1, 2, 3) dtype=int32_ref>,\n",
       " <tf.Variable 'other_variable:0' shape=(2,) dtype=int32_ref>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_variable:0' shape=(1, 2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'my_int_variable:0' shape=(1, 2, 3) dtype=int32_ref>,\n",
       " <tf.Variable 'other_variable:0' shape=(2,) dtype=int32_ref>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is possible to create a tensor that is not put into the `tf.GraphKeys.TRAINABLE_VARIABLES` by default; add it to the `tf.GraphKeys.LOCAL_VARIABLES` collection explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_local = tf.get_variable(\n",
    "    'my_local',\n",
    "    shape=(),\n",
    "    collections=[tf.GraphKeys.LOCAL_VARIABLES]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_variable:0' shape=(1, 2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'my_int_variable:0' shape=(1, 2, 3) dtype=int32_ref>,\n",
       " <tf.Variable 'other_variable:0' shape=(2,) dtype=int32_ref>,\n",
       " <tf.Variable 'my_local:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_local:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sooooo that doesn't work as advertised..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or pass flag `trainable=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_non_trainable = tf.get_variable(\n",
    "    \"my_non_trainable\",\n",
    "    shape=(),\n",
    "    trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_variable:0' shape=(1, 2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'my_int_variable:0' shape=(1, 2, 3) dtype=int32_ref>,\n",
       " <tf.Variable 'other_variable:0' shape=(2,) dtype=int32_ref>,\n",
       " <tf.Variable 'my_local:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_local:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whereas that is doing what i expected. I wonder if `LOCAL_VARIABLES` are just not trained, even though they are in `TRAINABLE_VARIABLES`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom collections are obviously supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.add_to_collection('my_collection', my_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_local:0' shape=() dtype=float32_ref>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection('my_collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### device placement\n",
    "\n",
    "variables can be pushed to specific devices using the `tf.device` context manager:\n",
    "\n",
    "```python\n",
    "with tf.device('/device:GPU:1'):\n",
    "    v = tf.get_variable('v', [1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a lot to remember and it's pretty important you get it right (what's the use of a GPU if you don't put the variables you use on it, or have them replicated across all cores?).\n",
    "\n",
    "`tensorflow` provides `tf.train.replica_device_setter` to automatically place variables in parameter servers for you:\n",
    "\n",
    "```python\n",
    "cluster_spec = {\n",
    "    \"ps\": [\"ps0:2222\", \"ps1:2222\"],\n",
    "    \"worker\": [\"worker0:2222\", \"worker1:2222\", \"worker2:2222\"]\n",
    "}\n",
    "with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):\n",
    "    v = tf.get_variable(\"v\", shape=[20, 20])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializing variables\n",
    "\n",
    "all variables must be initialized. for low-level / core api programs, you will need to do this explicitly. for higher-level apis (ex: `tf.contrib.slim`, `tf.estimator.Estimator`, `keras`), this is done for you automatically.\n",
    "\n",
    "explicit initialization might be a good idea if\n",
    "\n",
    "1. initialization is computationally expensive (e.g. reloading a checkpointed model)\n",
    "1. you're seeking deterministic behavior for a randomly initialized value in a distributed setting\n",
    "\n",
    "you can initialize all *trainable* variables using `tf.global_variables_initializer()`. in addition, every variable has an `initializer` attribute which is a per-variable initializer operation object.\n",
    "\n",
    "it is also possible to see which variables exist that have not been initialized at any given point in the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'my_variable' b'my_int_variable' b'other_variable' b'my_non_trainable'\n",
      " b'my_local']\n"
     ]
    }
   ],
   "source": [
    "utils.inspect(tf.report_uninitialized_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare this to the list when I allow for global variables to be initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'my_local']\n"
     ]
    }
   ],
   "source": [
    "utils.inspect(tf.report_uninitialized_variables(), init_global=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another tricky piece: `tf.global_variables_initializer` doesn't order variables in any particular way, so if there is codependence between them the user is required to initialize them in the correct order.\n",
    "\n",
    "the recommendation here seems to be to rely on the tensor-esque initializer syntax, directly passing the formula for the dependent tensor in as the initializer and using the `initialized_value` method of each variable to chain them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.get_variable('v', shape=(), initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.get_variable('w', initializer=v.initialized_value() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using variables\n",
    "\n",
    "treat it like any ol' tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = v + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign values using the `.assign`, `.assign_add`, etc methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = 0.0\n",
      "w = 1.0\n",
      "v = 1.0\n",
      "w = 2.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    assignment = v.assign_add(1)\n",
    "    print('v = {}'.format(v.eval()))\n",
    "    print('w = {}'.format(w.eval()))\n",
    "    sess.run(assignment)\n",
    "    print('v = {}'.format(v.eval()))\n",
    "    print('w = {}'.format(w.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the operations assocaited with these variables are special -- they are implemented in a way that makes GD / optimization easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, you can force an on-demand re-read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    assignment = v.assign_add(1)\n",
    "    with tf.control_dependencies([assignment]):\n",
    "        w = v.read_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sharing variable\n",
    "\n",
    "there are two ways of passing variables around\n",
    "\n",
    "1. explicitly: create a `python` variable in a shared `python` scope\n",
    "1. implicitly: add the varialbe to a `tf.variable_scope` object\n",
    "\n",
    "the second method is offered as a convenience (c.f. `tf.layer` and `tf.metrics` for examples of this method in use). these variable scopes play the same role as namespaces in base `python`: you will often want to create variable scopes to differentiate between variables that share similar names. the provided example is weights and biases in different layers of similar structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\n",
    "        \"weights\", \n",
    "        kernel_shape,\n",
    "        initializer=tf.random_normal_initializer()\n",
    "    )\n",
    "    \n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\n",
    "        \"biases\",\n",
    "        bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0)\n",
    "    )\n",
    "    \n",
    "    conv = tf.nn.conv2d(\n",
    "        input,\n",
    "        weights,\n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding='SAME'\n",
    "    )\n",
    "    \n",
    "    return tf.nn.relu(conv + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.random_normal([1,10,10,32])\n",
    "input2 = tf.random_normal([1,20,20,32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will succeed because it's the first time we're\n",
    "# creating the weights and biases vairables:\n",
    "x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-33-700f8a5e1cfb>\", line 6, in conv_relu\n    initializer=tf.random_normal_initializer()\n  File \"<ipython-input-35-2e6dcb6e8031>\", line 3, in <module>\n    x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ea86a63f0a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this will fail because it's the second time:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-700f8a5e1cfb>\u001b[0m in \u001b[0;36mconv_relu\u001b[0;34m(input, kernel_shape, bias_shape)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mkernel_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-33-700f8a5e1cfb>\", line 6, in conv_relu\n    initializer=tf.random_normal_initializer()\n  File \"<ipython-input-35-2e6dcb6e8031>\", line 3, in <module>\n    x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# this will fail because it's the second time:\n",
    "x = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can fix this by creating the variables within a different scope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_filter(input_images):\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])\n",
    "        \n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu(relu1, [5, 5, 32, 32], [32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_image_filter(input1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes we wish to re-use variables created in one scope later on. this can also be done in one of two ways. the first is with `reuse` of a context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"model\"):\n",
    "    output1 = my_image_filter(input1)\n",
    "    \n",
    "with tf.variable_scope(\"model\", reuse=True):\n",
    "    output2 = my_image_filter(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second is by explicitly taking the current scope (as aliased in the context manager expression) and calling the `reuse_variables` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"model2\") as scope:\n",
    "    output1 = my_image_filter(input1)\n",
    "    scope.reuse_variables()\n",
    "    output2 = my_image_filter(input2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out the name here to see that `model2` and `conv2` get reused as we build up our layers::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model2/conv2_1/Relu:0' shape=(1, 20, 20, 32) dtype=float32>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they also allow you to build a context from a previous variable scope object rather than just a name (good idea):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"model3\") as scope:\n",
    "    output1 = my_image_filter(input1)\n",
    "\n",
    "with tf.variable_scope(scope, reuse=True):\n",
    "    output2 = my_image_filter(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model3_1/conv2/Relu:0' shape=(1, 20, 20, 32) dtype=float32>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "this page was short and to the point\n",
    "\n",
    "+ variables are mutable tensors that can pass state around throughout your program\n",
    "+ they can be (and are, by default) put into *collections*\n",
    "    + you may define your own\n",
    "    + some default collections have pariticular influence in the greater architecture\n",
    "        + e.g. every variable in the `tf.GraphKeys.TRAINABLE_VARIABLES` collection will be initilized via the `tf.global_variables_initializer()` method\n",
    "+ speaking of which, variables must be initialized within each independent computation session\n",
    "+ variables can be explicitly pushed to different devices (gpu, tpu, etc)\n",
    "    + care must be taken to make sure they're on the right device\n",
    "    + some helper functions exist to make this easier\n",
    "    + be careful about dependencies between variables (dependent initialization must happen sequentially and doesn't do so by default)\n",
    "+ variables are used just like tensors\n",
    "+ variables can be shared\n",
    "    + explicitly (simplest)\n",
    "    + implicitly (via variable scopes)\n",
    "        + this allows reuse of names and more generally readable / reusable code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
