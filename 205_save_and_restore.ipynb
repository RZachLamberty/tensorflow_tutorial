{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save and restore\n",
    "\n",
    "following along [here](https://www.tensorflow.org/programmers_guide/saved_model)\n",
    "\n",
    "use `tf.train.Saver` to save and restore models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and restore variables\n",
    "\n",
    "best target for saving: variables (they already represent persistent state, now they persist between programs instead of just session runs). use `tf.train.Saver` to create a `save` and `restore` operation and then execute these.\n",
    "\n",
    "tensorflow save files have a \"binary checkpoint\" format mapping variable names to tensor values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save variables\n",
    "\n",
    "`tf.train.Saver.save` is the goto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some tensors as outputs of the assign operation\n",
    "inc_v1 = v1.assign(v1 + 1)\n",
    "dec_v2 = v2.assign(v2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an op to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Later, launch the model, initialize the variables, do some work, and save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Do some work with the model.\n",
    "    inc_v1.op.run()\n",
    "    dec_v2.op.run()\n",
    "    \n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "    \n",
    "    print(\"Model saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\r\n",
      "-rw-r--r-- 1 zlamberty   87 Jul  3 18:50 checkpoint\r\n",
      "-rw-r--r-- 1 zlamberty   32 Jul  3 18:50 model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 zlamberty  143 Jul  3 18:50 model.ckpt.index\r\n",
      "-rw-r--r-- 1 zlamberty 3931 Jul  3 18:50 model.ckpt.meta\r\n",
      "drwx------ 2 root      4096 Apr 28 00:42 \u001b[0m\u001b[01;34mtmp51j6z2lb_kernels\u001b[0m/\r\n",
      "drwx------ 2 root      4096 Jun 14 19:00 \u001b[01;34mtmpgk_8bk3y_kernels\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ll /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"/tmp/model.ckpt\"\r\n",
      "all_model_checkpoint_paths: \"/tmp/model.ckpt\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick asside on nuking the default graph\n",
    "\n",
    "how did I not see `tf.reset_default_graph()` before now??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.24547701799398203&quot;).pbtxt = 'node {\\n  name: &quot;v1/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 3\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v1&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 3\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v1/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;v1&quot;\\n  input: &quot;v1/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v1/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;v1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v2/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 5\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v2&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 5\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v2/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;v2&quot;\\n  input: &quot;v2/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;v2/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;v2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;v1/read&quot;\\n  input: &quot;add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;v1&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;v2/read&quot;\\n  input: &quot;sub/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;v2&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^v1/Assign&quot;\\n  input: &quot;^v2/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;v1&quot;\\n        string_val: &quot;v2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;v1&quot;\\n  input: &quot;v2&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;v1&quot;\\n        string_val: &quot;v2&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;v1&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v1&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;v2&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@v2&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.24547701799398203&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.8097517778113655&quot;).pbtxt = '';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.8097517778113655&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restore variables\n",
    "\n",
    "we saved 'em. big whoop. show me how to restore them and I'll *really* be impressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well I'll be damned, first time I've seen this, and it's pretty goddamn useful\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "v2 = tf.get_variable(\"v2\", shape=[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n",
      "v1 : [1. 1. 1.]\n",
      "v2 : [-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # Check the values of the variables\n",
    "    print(\"v1 : %s\" % v1.eval())\n",
    "    print(\"v2 : %s\" % v2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose variables to save and restore\n",
    "\n",
    "by default, `tf.train.Saver` will save *all* variables in the graph, and will save them with a key being the name of the variable as it was created\n",
    "\n",
    "+ want different keys / names? want to pick up a variable named `x` but call it `y` this time? provided example: `weights --> params`.\n",
    "+ want to save only one of a berjuilion variables?\n",
    "\n",
    "`tf.train.Saver` takes a list or mapping of variable names to pay attention to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "v1 : [0. 0. 0.]\n",
      "v2 : [-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", [3], initializer = tf.zeros_initializer)\n",
    "v2 = tf.get_variable(\"v2\", [5], initializer = tf.zeros_initializer)\n",
    "\n",
    "# Add ops to save and restore only `v2` using the name \"v2\"\n",
    "saver = tf.train.Saver({\"v2\": v2})\n",
    "\n",
    "# Use the saver object normally after that.\n",
    "with tf.Session() as sess:\n",
    "    # Initialize v1 since the saver will not.\n",
    "    v1.initializer.run()\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "  \n",
    "    print(\"v1 : %s\" % v1.eval())\n",
    "    print(\"v2 : %s\" % v2.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, in the above, we told `saver` to only care about `v2`, and lo! only `v2` was restored when we called `saver.restore`. not bad.\n",
    "\n",
    "good idea: we can have multiple savers for different contexts / intentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect variables in a checkpoint\n",
    "\n",
    "you can inspect variables in a checkpoint with `tf.inspect_checkpoint`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the inspect_checkpoint library\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkp.print_tensors_in_checkpoint_file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  v1\n",
      "[1. 1. 1.]\n",
      "tensor_name:  v2\n",
      "[-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# print all tensors in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\n",
    "    file_name=\"/tmp/model.ckpt\",\n",
    "    tensor_name='',\n",
    "    all_tensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  v1\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# print only tensor v1 in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\n",
    "    \"/tmp/model.ckpt\",\n",
    "    tensor_name='v1',\n",
    "    all_tensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  v2\n",
      "[-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# print only tensor v2 in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file(\n",
    "    \"/tmp/model.ckpt\",\n",
    "    tensor_name='v2',\n",
    "    all_tensors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and restore models\n",
    "\n",
    "variables are cool and all but what bout dem models boi?\n",
    "\n",
    "claim: `SavedModel` is used to save and load models. this is meant to be language-neutral. there are several methods of interacting with `SavedModel` discussed below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build and load a `SavedModel`\n",
    "\n",
    "### simple save\n",
    "\n",
    "just use `tf.saved_model.simple_save` *a la*\n",
    "\n",
    "```python\n",
    "tf.saved_model.simple_save(\n",
    "    session,\n",
    "    export_dir,\n",
    "    inputs={'x': x, 'y': y},\n",
    "    outputs={'z': z}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this presupposes the model is built and defined with the stated inputs and outputs. my assumption is that the entire intermediary graph is then persisted intact, and all you need to know on re-instantiation is that it will expect those inputs and emit those outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually build a `SavedModel`\n",
    "\n",
    "an alternative for more complicated scenarios (not sure what those scenarios are yet) is to use the `tf.saved_model.builder.SavedModelBuilder` class. this class will save `MetaGraphDef` objects: objects which represent a dataflow graph plus variables, assets, and \"signatures\" (inputs and outputs, i.e. function signatures). this is a protocol buffer repr of a regular `MetaGraph`.\n",
    "\n",
    "example use:\n",
    "\n",
    "```python\n",
    "export_dir = ...\n",
    "\n",
    "# do stuff\n",
    "# ...\n",
    "    \n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # do stuff\n",
    "    # ...\n",
    "    \n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess,\n",
    "        [tag_constants.TRAINING],\n",
    "        signature_def_map=foo_signatures,\n",
    "        assets_collection=foo_assets,\n",
    "        strip_default_attrs=True\n",
    "    )\n",
    "\n",
    "# Add a second MetaGraphDef for inference.\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # do stuff\n",
    "    # ...\n",
    "    \n",
    "    builder.add_meta_graph([tag_constants.SERVING], strip_default_attrs=True)\n",
    "\n",
    "# do stuff\n",
    "# ...\n",
    "\n",
    "builder.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward compatibility via `strip_default_attrs = True`\n",
    "\n",
    "says what is means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading a `SavedModel` in `python`\n",
    "\n",
    "assuming a model has been saved using the `SavedModelBuilder` as above. we can load from those saved models using the items associated with any set of tags:\n",
    "\n",
    "```python\n",
    "export_dir = ...\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load a `SavedModel` in `c++`\n",
    "\n",
    "rare moment of c code creepin in\n",
    "\n",
    "```c++\n",
    "const string export_dir = ...\n",
    "SavedModelBundle bundle;\n",
    "...\n",
    "LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},\n",
    "               &bundle);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and serve a `SavedModel` in `tensorflow` serving\n",
    "\n",
    "the `tensorflow_model_server` package comes with a `cli` for serving `SavedModel` objects from file. as long as they all live in directoies `{model_base_bath}/{numeric_index}`, they will be loaded when the value of `--model_base_path` is set as that containing directory.\n",
    "\n",
    "their example: models save to `/tmp/model/0001` and `/tmp/model/0002` serve via the call\n",
    "\n",
    "```bash\n",
    "tensorflow_model_server --port=port-numbers --model_name=your-model-name --model_base_path=your_model_base_path\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standard constants\n",
    "\n",
    "there are a lot of built-in constants living in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GPU', 'SERVING', 'TPU', 'TRAINING']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_ for _ in dir(tf.saved_model.tag_constants) if _[:2] != '__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLASSIFY_INPUTS',\n",
       " 'CLASSIFY_METHOD_NAME',\n",
       " 'CLASSIFY_OUTPUT_CLASSES',\n",
       " 'CLASSIFY_OUTPUT_SCORES',\n",
       " 'DEFAULT_SERVING_SIGNATURE_DEF_KEY',\n",
       " 'PREDICT_INPUTS',\n",
       " 'PREDICT_METHOD_NAME',\n",
       " 'PREDICT_OUTPUTS',\n",
       " 'REGRESS_INPUTS',\n",
       " 'REGRESS_METHOD_NAME',\n",
       " 'REGRESS_OUTPUTS']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_ for _ in dir(tf.saved_model.signature_constants) if _[:2] != '__']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea is that these are common tags you might use in the arguments to `tf.saved_model` functions, so keep them normalized and generally accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using `SavedModel` with `Estimator`s\n",
    "\n",
    "elsewhere we've covered `Estimator` objects; here we show how they interact with `SavedModel` objects. the use case is pretty obvious -- we've trained a good model, now let's use it to make scores / predictions.\n",
    "\n",
    "the idea, then, is to take that trained estimator and to use it to create a new object which is an encapsulated model *service*. the format for that model service is an assumed standard (I have never heard of it so I will have to trust tf on this one): the `SavedModel` format\n",
    "\n",
    "the steps:\n",
    "\n",
    "1. specify output nodes and apis\n",
    "1. export model to `SavedModel` format\n",
    "1. serve model and request predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare serving inputs\n",
    "\n",
    "when you create a training estimator you have to write an `input_fn` to ingest and preprocess data. it would seem easy enough to re-use this, but for reasons beyond me you actually have a separate function for serving: `serving_input_receiver_fn`.\n",
    "\n",
    "the `serving_input_receiver_fn` ingests and preprocesses inference requests, returning a `tf.estimator.export.ServingInputReceiver`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical usage pattern:\n",
    "\n",
    "1. we start by knowning that the server will eventually receive inference request\n",
    "    1. it is a serialized `tf.Example`\n",
    "1. `serving_input_receiver_fn` creates a string placeholder which can take on the value of the serialized inference request\n",
    "1. `serving_input_receiver_fn` parses `tf.Example` with `tf.parse_example` operations\n",
    "    1. this requires a parsing specification (how do I take what I need out of `tf.Example`?)\n",
    "        1. a parsing specification is a lot like a feature dict -- it has keys that are feature names and then `tf.FixedLenFeature`, `tf.VarLenFeature`, or `tf.SparseFeature`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a full example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a feature specification is required for the `tf.parse_example`\n",
    "# call, which is an operation that eats a string (e.g. a \n",
    "# string-serialized protobuff of a scorable record, sent over grpc)\n",
    "# and converts it into a tensor of fixed dtype and shape\n",
    "feature_spec = {\n",
    "    'foo': tf.FixedLenFeature(dtype=tf.float32, shape=[4]),\n",
    "    'bar': tf.VarLenFeature(dtype=tf.float32)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_batch_size = 32\n",
    "\n",
    "def serving_input_receiver_fn():\n",
    "    \"\"\"An input receiver that expects a serialized tf.Example.\"\"\"\n",
    "    # the placeholder tensor which will eventually hold the serialized infernece request\n",
    "    serialized_tf_example = tf.placeholder(\n",
    "        dtype=tf.string,\n",
    "        shape=[default_batch_size],\n",
    "        name='input_example_tensor'\n",
    "    )\n",
    "    \n",
    "    # a feature dict\n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    \n",
    "    # features here is built out of the placeholder and the feature_spec we defined above\n",
    "    features = tf.parse_example(serialized_tf_example, feature_spec)\n",
    "    \n",
    "    # the feature dict plus features come togehter to give us the input receiver tensor\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ServingInputReceiver(features={'bar': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f32bffefcf8>, 'foo': <tf.Tensor 'ParseExample_3/ParseExample:3' shape=(32, 4) dtype=float32>}, receiver_tensors={'examples': <tf.Tensor 'input_example_tensor_3:0' shape=(32,) dtype=string>}, receiver_tensors_alternatives=None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_input_receiver_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above is only necessary if the input to your serving process is the serialized protobuff example records. if you are serving things locally, they will exist in \"raw\" in-memory form, and you can consume them directly. **even then**, you need to create a placeholder to be updated, and you still must provide a `serving_input_receiver_fn` to do this.\n",
    "\n",
    "they recommend using the built-in `tf.estimator.export.build_raw_serving_input_receiver_fn` for this use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more concrete way of discussing all this: when serving a model, you have fixed inputs to the model, but not necessarily fixed inputs to the interface of the serving process. therefore, tensorflow requires you define a function -- even an extremely simple one -- to perform the transform stage of that ETL from received input to model input, and this function is called the `serving_input_receiver_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform the export\n",
    "\n",
    "the work we did in the last section was to define how a `SavedModel` object should expect recieved inference requests to be communicated (raw tensors? serialized example records?) and then how it should convert them to the model's expected input.\n",
    "\n",
    "given a trained model and this `serving_input_receiver_fn` set of operations/tensors, we can now save the combination together as a `SavedModel` via the cleverly named export function `tf.estimator.Estimator.export_savedmodel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function call will typically look like:\n",
    "\n",
    "```python\n",
    "estimator.export_savedmodel(\n",
    "    export_dir_base,  # where the model will be saved\n",
    "    serving_input_receiver_fn,  # the function we defined above\n",
    "    strip_default_attrs=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specify the outputs of a custom model\n",
    "\n",
    "in a previous notebook (007_creating_custom_estimators) we defined a custom model. defining a custom estimator is basically equivalent to defining a custom `model_fn`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def my_model_fn(features, labels, mode, params):\n",
    "    # 1. build the model\n",
    "    net = tf.feature_column.input_layer(features=features,\n",
    "                                        feature_columns=params['feature_columns'])\n",
    "    \n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "    \n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "    \n",
    "    # 2. define mode behavior\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "    # 2.a. PREDICT\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'class_ids': predicted_classes[:, tf.newaxis],\n",
    "                       'probabilities': tf.nn.softmax(logits),\n",
    "                       'logits': logits}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    # the remaining two modes both require loss and accuracy calculations\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "        \n",
    "    # 2.b. EVAL\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode,\n",
    "                                          loss=loss,\n",
    "                                          eval_metric_ops={'accuracy': accuracy})\n",
    "    \n",
    "    # 2.c. TRAIN\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    raise ValueError(\"mode must be one of tf.estimator.ModeKeys.{PREDICT,EVAL,TRAIN}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that in the above, what our `model_fn` is *actually* returning is a `tf.estimator.EstimatorSpec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.EstimatorSpec.__new__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one of the parameters to `tf.estimator.EstimatorSpec` is `export_outputs`. this argument is explicitly related to the `SavedModel` format and defines the type of output that will be produced by this model function in the provided `mode`.\n",
    "\n",
    "to steal directly from the documentation:\n",
    "\n",
    "> ```\n",
    "export_outputs: Describes the output signatures to be exported to\n",
    "    `SavedModel` and used during serving.\n",
    "    A dict `{name: output}` where:\n",
    "    * name: An arbitrary name for this output.\n",
    "    * output: an `ExportOutput` object such as `ClassificationOutput`,\n",
    "        `RegressionOutput`, or `PredictOutput`.\n",
    "    Single-headed models only need to specify one entry in this dictionary.\n",
    "    Multi-headed models should specify one entry for each head, one of\n",
    "    which must be named using\n",
    "    signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, what's that mean?\n",
    "\n",
    "basically, the tensorflow model serving api is tightly defined, and you have to map the desired output type for your model to one of the defined api methods. the three output types (`Classification`, `Regression`, and `Predict`) will correspond to different api endpoints. also, if there are multiple models being considered / served, the `name` in the `export_outputs` dictionary will also be used in the api call to differentiate between models / pick which model we is served for any given api call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serve the exports model locally\n",
    "\n",
    "for development you can run the opensource tensorflow serving. you build it with `bazel`. it seems like a pain.\n",
    "\n",
    "also looks like there's a `docker` image for ya: https://www.tensorflow.org/serving/docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### request predictions from a local server\n",
    "\n",
    "there is some basic information here about how the service is built and how `python` programs can invoke it.\n",
    "\n",
    "for example, in order to have `python` protobuffs downloaded and available for the service to use, you must inform the `bazel` build tool of those dependencies ahead of time by providing the dependency list:\n",
    "\n",
    "```python\n",
    "deps = [\n",
    "    # optional, often not all will be required / defined\n",
    "    \"//tensorflow_serving/apis:classification_proto_py_pb2\",\n",
    "    \"//tensorflow_serving/apis:regression_proto_py_pb2\",\n",
    "    \"//tensorflow_serving/apis:predict_proto_py_pb2\",\n",
    "    # required for the whole durn thing\n",
    "    \"//tensorflow_serving/apis:prediction_service_proto_py_pb2\",\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a service that installs those dependencies will be accessible through `python` code with officially maintained api libraries:\n",
    "\n",
    "```python\n",
    "from tensorflow_serving.apis import classification_pb2\n",
    "from tensorflow_serving.apis import regression_pb2\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in this instance, your `SavedModel` service will be exposed over `rpc` and your `python` code can make simple direct connections to that service for classification, regression, prediction, etc.\n",
    "\n",
    "here's an example program for interacting with a service:\n",
    "\n",
    "```python\n",
    "# library required to build grpc protobuff messages\n",
    "from grpc.beta import implementations\n",
    "\n",
    "# create the connection to the service\n",
    "channel = implementations.insecure_channel(host, int(port))\n",
    "stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "# create an empty protobuff request and then add images to it\n",
    "request = classification_pb2.ClassificationRequest()\n",
    "example = request.input.example_list.examples.add()\n",
    "example.features.feature['x'].float_list.value.extend(image[0].astype(float))\n",
    "\n",
    "# make the classification request\n",
    "result = stub.Classify(request, 10.0)  # 10 secs timeout\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cli to inspect and execute `SavedModel`\n",
    "\n",
    "simply cli for sanity checks (e.g. introspection of inputs and outputs)\n",
    "\n",
    "### install the `SavedModel` cli\n",
    "\n",
    "the `SavedModel` cli is installed on this system by virtue of being packaged with the `tensorflow` `docker` base image. we would possibly have had to install it separately under differenc circumstances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overview of commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "usage: saved_model_cli [-h] [-v] {show,run,scan} ...\n",
      "\n",
      "saved_model_cli: Command-line interface for SavedModel\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help       show this help message and exit\n",
      "  -v, --version    show program's version number and exit\n",
      "\n",
      "commands:\n",
      "  valid commands\n",
      "\n",
      "  {show,run,scan}  additional help\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as it says: `show`, `run`, and `scan` exist. docs only cover the first two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "usage: saved_model_cli show [-h] --dir DIR [--all] [--tag_set TAG_SET]\n",
      "                            [--signature_def SIGNATURE_DEF_KEY]\n",
      "\n",
      "Usage examples:\n",
      "To show all tag-sets in a SavedModel:\n",
      "$saved_model_cli show --dir /tmp/saved_model\n",
      "\n",
      "To show all available SignatureDef keys in a MetaGraphDef specified by its tag-set:\n",
      "$saved_model_cli show --dir /tmp/saved_model --tag_set serve\n",
      "\n",
      "For a MetaGraphDef with multiple tags in the tag-set, all tags must be passed in, separated by ';':\n",
      "$saved_model_cli show --dir /tmp/saved_model --tag_set serve,gpu\n",
      "\n",
      "To show all inputs and outputs TensorInfo for a specific SignatureDef specified by the SignatureDef key in a MetaGraph.\n",
      "$saved_model_cli show --dir /tmp/saved_model --tag_set serve --signature_def serving_default\n",
      "\n",
      "To show all available information in the SavedModel:\n",
      "$saved_model_cli show --dir /tmp/saved_model --all\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --dir DIR             directory containing the SavedModel to inspect\n",
      "  --all                 if set, will output all information in given SavedModel\n",
      "  --tag_set TAG_SET     tag-set of graph in SavedModel to show, separated by ','\n",
      "  --signature_def SIGNATURE_DEF_KEY\n",
      "                        key of SignatureDef to display input(s) and output(s) for\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are also some interesting examples provided:\n",
    "\n",
    "```sh\n",
    "$ saved_model_cli show --dir /tmp/saved_model_dir\n",
    "The given SavedModel contains the following tag-sets:\n",
    "serve\n",
    "serve, gpu\n",
    "```\n",
    "\n",
    "```sh\n",
    "$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve\n",
    "The given SavedModel `MetaGraphDef` contains `SignatureDefs` with the\n",
    "following keys:\n",
    "SignatureDef key: \"classify_x2_to_y3\"\n",
    "SignatureDef key: \"classify_x_to_y\"\n",
    "SignatureDef key: \"regress_x2_to_y3\"\n",
    "SignatureDef key: \"regress_x_to_y\"\n",
    "SignatureDef key: \"regress_x_to_y2\"\n",
    "SignatureDef key: \"serving_default\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `run` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "usage: saved_model_cli run [-h] --dir DIR --tag_set TAG_SET --signature_def\n",
      "                           SIGNATURE_DEF_KEY [--inputs INPUTS]\n",
      "                           [--input_exprs INPUT_EXPRS]\n",
      "                           [--input_examples INPUT_EXAMPLES] [--outdir OUTDIR]\n",
      "                           [--overwrite] [--tf_debug]\n",
      "\n",
      "Usage example:\n",
      "To run input tensors from files through a MetaGraphDef and save the output tensors to files:\n",
      "$saved_model_cli show --dir /tmp/saved_model --tag_set serve \\\n",
      "   --signature_def serving_default \\\n",
      "   --inputs input1_key=/tmp/124.npz[x],input2_key=/tmp/123.npy \\\n",
      "   --input_exprs 'input3_key=np.ones(2)' \\\n",
      "   --input_examples 'input4_key=[{\"id\":[26],\"weights\":[0.5, 0.5]}]' \\\n",
      "   --outdir=/out\n",
      "\n",
      "For more information about input file format, please see:\n",
      "https://www.tensorflow.org/programmers_guide/saved_model_cli\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --dir DIR             directory containing the SavedModel to execute\n",
      "  --tag_set TAG_SET     tag-set of graph in SavedModel to load, separated by ','\n",
      "  --signature_def SIGNATURE_DEF_KEY\n",
      "                        key of SignatureDef to run\n",
      "  --inputs INPUTS       Loading inputs from files, in the format of '<input_key>=<filename>, or '<input_key>=<filename>[<variable_name>]', separated by ';'. The file format can only be from .npy, .npz or pickle.\n",
      "  --input_exprs INPUT_EXPRS\n",
      "                        Specifying inputs by python expressions, in the format of \"<input_key>='<python expression>'\", separated by ';'. numpy module is available as 'np'. Will override duplicate input keys from --inputs option.\n",
      "  --input_examples INPUT_EXAMPLES\n",
      "                        Specifying tf.Example inputs as list of dictionaries. For example: <input_key>=[{feature0:value_list,feature1:value_list}]. Use \";\" to separate input keys. Will override duplicate input keys from --inputs and --input_exprs option.\n",
      "  --outdir OUTDIR       if specified, output tensor(s) will be saved to given directory\n",
      "  --overwrite           if set, output file will be overwritten if it already exists.\n",
      "  --tf_debug            if set, will use TensorFlow Debugger (tfdbg) to watch the intermediate Tensors and runtime GraphDefs while running the SavedModel.\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli run -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are several supported ways of passing tensors to the saved model:\n",
    "\n",
    "1. `--inputs`: accepts files in `npy`, `npz`, or `pkl` format\n",
    "    1. e.g. `--inputs x=x.npy`\n",
    "    1. e.g. `--inputs x=xyz.pkl[x]`\n",
    "1. `--input_exprs`: accepts full `python` expressions\n",
    "    1. e.g. `--input_exprs x=[[1],[2],[3]]`\n",
    "    1. e.g. `--input_exprs x=np.ones((32,32,3))`\n",
    "1. `--input_examples`: accepts tensorflow examples as fully written out strings obeying the `tf.train.Example` protobuff specification\n",
    "    1. e.g. `--input_examples x=[{\"age\":[22,24],\"education\":[\"BS\",\"MS\"]}]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structure of a `SavedModel` directory\n",
    "\n",
    "basic discussion of the contents in the directory created when you save a model:\n",
    "\n",
    "```\n",
    "assets/\n",
    "assets.extra/\n",
    "variables/\n",
    "    variables.data-?????-of-?????\n",
    "    variables.index\n",
    "saved_model.pb|saved_model.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "whew. a lot.\n",
    "\n",
    "1. there are a few ways of saving and restoring things, depending on what you're saving\n",
    "    1. *variables* can be saved with `tf.train.Saver` \n",
    "    1. *models* are saved as `SavedModel` objects\n",
    "1. variable saving (via `tf.train.Saver`) is extremely straight-forward\n",
    "    1. create a `tf.train.Saver` object which points to a specific directory / filename prefix\n",
    "    1. create `save` or `restore` operations\n",
    "    1. execute those operations within some session\n",
    "1. what is a \"model\" here?\n",
    "    1. a \"model\" here is defined as the \"variables, the graph, and the graph's metadata\"\n",
    "    1. models are saved in a language-independent format called `SavedModel`\n",
    "1. saving models\n",
    "    1. the exact way we save models depends on the use case\n",
    "        1. saving so we can re-open in `python` or `c++`\n",
    "            1. simple, support \"only\" the `Predict` api: save using `tf.saved_model.simple_save`\n",
    "                1. docs go out of their way to explain that this will only support the `Predict` api, not sure what that excludes\n",
    "            1. beyond that: create a `tf.saved_model.builder.SavedModelBuilder` object and get explicit with it\n",
    "1. saving `Estimators` such that we can feed them into a `tensorflow_server`\n",
    "    1. this is a *huge* digression into exacly how this is done. basic steps:\n",
    "        1. add a few special items to the already-trained estimator:\n",
    "            1. a function which determines the input of records to the service and transforms them to the expected input type of the model\n",
    "            1. if the estimator is custom, define the output types `export_outputs` dictionary as part of the `model_fn` definition\n",
    "        1. use the estimator's `estimator.export_savedmodel` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
